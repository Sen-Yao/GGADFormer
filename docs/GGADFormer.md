# Introduction

图异常检测（Graph Anomaly Detection）的核心任务是识别在图结构数据中行为模式或特征显著偏离大部分节点的实体。这些异常通常表现为结构性异常（如孤立点或意外的社群连接）、属性异常（节点的特征向量与众不同），亦或是两者的结合。

在众多现实场景中，标签稀缺是一个普遍挑战，由此催生了半监督图异常检测（Semi-supervised Graph Anomaly Detection）这一重要研究方向。该设定下，模型在训练时仅能利用极少数已知的“正常”节点作为先验知识，而绝大部分正常节点与所有的异常节点均是无标签的。

考虑到真实世界应用（如金融风控、网络安全入侵检测）中数据规模庞大、异常模式未知多变，使得全面标注的成本极其高昂。与之相对，获取少量正常样本的成本则低廉得多。因此，半监督图异常检测范式不仅具备高度的普适性，更是在资源受限的实际环境中解决问题的关键技术，具有重要的理论研究与实际应用价值。

在半监督图异常检测设定下，模型面临两大核心困境。其一，训练信号极度稀疏：模型仅能从有限的正常样本中归纳“正常”模式的分布，而对“异常”模式则完全未知，这极易导致模型对正常模式的认知产生偏差（bias），形成过于狭隘的决策边界，从而引发过拟合。其二，现有图神经网络（GNN）在处理大规模图时，普遍受限于过平滑与可扩展性（Scalability）的瓶颈。

为系统性地应对上述挑战，我们设计并提出了GGADFormer，一个基于 Transformer 架构的生成式图异常检测框架。GGADFormer 的先进性主要体現在其四大核心贡献：1) 一种解耦式的图结构感知输入编码策略，它将图拓扑信息的提取与节点表征学习分离，使得模型能够进行高效的小批量训练，天然地适用于大规模图；2) 图结构感知的输入编码策略，防止过度平滑；3) 一种新颖的生成式策略，在缺乏真实异常样本的情况下，**人工合成高质量的伪异常样本**；4) 多重学习机制，确保模型学习到鲁棒且具有区分性的节点表征。

GGADFormer 的创新点包括

- 首个基于 Transformer 的生成式半监督图异常检测模型
- 支持 mini-batch 训练，对大图有良好的可扩展性
- 相比传统的半监督异常检测方法采用的 15% 的划分比，GGADFormer 可以在 5% 的划分比的场景下达到非常优秀的性能

# 模型架构

在此节我们将介绍 GGADFormer 的各个组成部分。

### 图结构感知的 Tokenization

传统的图神经网络（如 GCN）通过邻居聚合来学习节点表示，但这通常会导致过度平滑（over-smoothing）问题，即随着传播层数增加，节点的表示趋于同质化，难以区分，这对需要捕捉细微差异的异常检测任务是致命的。

为解决此问题并让 Transformer 感知到图结构信息，我们采用一种基于传播机制来编码节点的拓扑特征。

$$X_p^{k+1} = (1-\alpha) \cdot \mathbf{A} \cdot X_p^{k} + \alpha \cdot \mathbf{X}_0$$

其中，$\mathbf{A}$ 是归一化后的邻接矩阵，$\mathbf{X}_0$ 是节点的原始特征。经过 $k$ 次传播后，得到了一系列的 token sequence $X_0, X_p^1, X_p^2,\cdots, X_p^k$，此序列融入了节点邻居的结构信息，同时超参数 $\alpha$ 保证了节点自身的原始特征不会被完全稀释，从而有效缓解了过度平滑问题，保留了节点的独特性。

token sequence 保留了用于对应节点用于异常检测所需要的所有图上下文信息，因此 Transformer 仅需要处理此序列，不需要计算全图中的节点信息，大大降低了计算开销。

## 基于跨空间重构误差引导的伪异常生成

在半监督学习中，尤其当已标记的正常节点极为稀疏时，单纯依赖对比学习机制的模型面临着学习“捷径解”的风险。模型可能仅学会区分有限的训练样本，而未能泛化至对“正常性”这一抽象概念的全面理解。为了应对这一挑战并从根本上提升节点表征的质量，我们设计了一个基于自编码器思想的协同框架，它不仅作为一种强大的**表征正则化 (Representation Regularization)** 手段，更创新性地扮演了**伪异常生成引导者 (Outlier Generation Guidance)** 的角色。

该框架的核心在于一个**编码器-解码器**结构。我们的Transformer编码器 $\mathcal{E}$ 负责将输入令牌 $\mathbf{t}_i$ (包含节点 $v_i$ 的原始及结构化特征) 压缩成一个低维、稠密的嵌入表征 $\mathbf{h}_i = \mathcal{E}(\mathbf{t}_i)$。我们额外设计了一个解码器 $\mathcal{D}_{tok}$，其任务是从嵌入 $\mathbf{h}_i$ “还原”回原始的输入令牌 $\hat{\mathbf{t}}_i = \mathcal{D}_{tok}(\mathbf{h}_i)$。我们认为，一个高质量、信息丰富的节点嵌入，理应蕴含足够的信息来还原其自身的全部构成。由于训练数据主要由正常节点构成，最小化重构误差的过程会“迫使”编码器 $\mathcal{E}$ 去学习和提炼正常模式最本质、最关键的特征，自然地过滤掉随机噪声，从而使最终的嵌入 $\mathbf{h}_i$ 成为对“正常性”的高度浓缩和纯化的表达。

然而，此重构机制的价值远不止于表征正则化。其产生的**重构误差向量**，通常被视为一个待优化的标量损失，实则蕴藏了用以生成高质量伪异常的关键信息。我们理论的出发点是**流形假设**，即正常节点的数据分布在嵌入空间中构成一个低维的“正常性流形”。解码器 $\mathcal{D}_{tok}$ 的存在隐式地定义了这个流形。对于一个正常节点 $v_i$，其在令牌空间的重构误差向量被定义为：
$$ \mathbf{e}^{(tok)}_i = \mathcal{D}_{tok}(\mathbf{h}_i) - \mathbf{t}_i $$
该向量 $\mathbf{e}^{(tok)}_i$ 精确地指明了在模型的当前认知下，样本 $v_i$ 的哪些方面最不符合其学到的“通用正常模式”。它是在高维、结构化的令牌空间中对“异常倾向”的直接量化。

为了在低维、稠密的嵌入空间中利用这一信息，我们引入了一个可学习的线性**投影层** $\mathcal{P}: \mathbb{R}^{d_{token}} \to \mathbb{R}^{d_{emb}}$，它负责将令牌空间中的结构化误差向量 $\mathbf{e}^{(tok)}_i$ **“语义提升”** (semantically lift) 为嵌入空间中的一个有意义的扰动方向 $\mathbf{e}^{(emb)}_i$：
$$ \mathbf{e}^{(emb)}_i = \mathcal{P}(\mathbf{e}^{(tok)}_i) $$
最终，我们通过将一个正常节点的原始嵌入 $\mathbf{h}_i$，沿着这个由模型自身指认的“最可疑”的异常方向 $\mathbf{e}^{(emb)}_i$ 进行扰动，来合成其对应的伪异常嵌入 $\tilde{\mathbf{h}}_i$：
$$ \tilde{\mathbf{h}}_i = \mathbf{h}_i + \alpha \cdot \mathbf{e}^{(emb)}_i $$
其中，$\alpha$ 是一个超参数，用于控制所生成异常的强度。这种生成方式生成的不是随机的或基于简单启发式规则的负样本，而是针对模型当前认知边界的**自适应硬负样本 (Adaptive Hard-Negative Samples)**。通过这种方式，自编码器框架形成了一个协同增强的闭环：解码器在作为正则化器的同时，主动地为对比学习模块提供了最具挑战性的学习信号，从而驱动整个模型学习一个更加鲁棒和紧凑的正常类别边界。

## 中心点对齐：构建可信异常边界

尽管我们设计的生成策略能够创造出与正常样本显著不同的伪异常点，但若不对其在嵌入空间中的位置加以约束，优化过程可能会陷入一个“捷径解”：模型仅仅学会将伪异常点的嵌入推向距正常点簇无穷远的位置，从而轻易地实现分离。这种无约束的分离虽然能降低损失，但会导致模型学习到一个松散、泛化能力差的决策边界，而未能精确刻画“正常”与“异常”之间那条微妙而关键的界线。

为了解决这一问题，我们引入了**中心点对齐 (Central Point Alignment)** 机制，其核心目标是强制所有生成的伪异常样本，落在一个以正常点簇中心为球心、预设半径为界的**可信异常超球 (Credible Anomaly Hypersphere)** 内。

首先，我们通过计算训练批次中所有正常节点嵌入的均值，来动态地确定正常点簇的**原型中心 (prototypical center)** $\mathbf{c}$：
$$ \mathbf{c} = \frac{1}{|V_{norm}|} \sum_{v_i \in V_{norm}} \mathbf{h}_i $$
其中，$V_{norm}$ 是当前批次中的正常节点集合，$\mathbf{h}_i$ 是其对应的嵌入。这个中心点 $\mathbf{c}$ 代表了模型在当前状态下对“普遍正常性”的凝聚表达。

接着，我们定义一个基于边距 (margin) 的对齐损失 $L_{CPA}$。对于每一个生成的伪异常嵌入 $\tilde{\mathbf{h}}_j$，我们计算其到中心点 $\mathbf{c}$ 的欧氏距离，并惩罚那些超出预设信心边距 $R$ (confidence margin) 的样本：
$$ L_{CPA} = \frac{1}{|\tilde{V}|} \sum_{\tilde{\mathbf{h}}_j \in \tilde{V}} \max(0, \ \|\tilde{\mathbf{h}}_j - \mathbf{c}\|_2 - R) $$
其中, $\tilde{V}$ 是生成的伪异常样本集合。此损失函数具有明确的几何意义：当一个伪异常点位于以 $\mathbf{c}$ 为中心、半径为 $R$ 的超球内部或表面时（$\|\tilde{\mathbf{h}}_j - \mathbf{c}\|_2 \le R$），不施加任何惩罚；一旦其越出该边界，损失将随其距离的增加而线性增长。

通过这种方式，“中心点对齐”机制确保了生成的伪异常样本始终处于一个“可信”的异常区域内。它们既不会因与正常点簇过于接近而失去作为负样本的价值，也不会因被推到无限远处而成为优化过程中的平凡样本。相反，它们被约束在决策边界附近，为模型提供了最具挑战性与信息量的学习信号，从而迫使模型学习到一个更加紧凑、精确且鲁棒的异常检测边界。


## Experiments

我们在 `Amazon`, `photo`, `reddit`, `ellptic`, `t_finance`, `tolokers`, `questions` 等多个不同的图异常检测常用的数据集上进行了试验。

数据集划分上，我们仅采用 5% 的节点作为训练集，模型仅能访问训练集中的正常节点的标签。

### 实验对比

我们使用了 5 个随机种子进行了实验，实验结果如下：

AUC:


|Dataset|Amazon|Reddit|photo|elliptic|t_finance|tolokers|questions
|-|-|-|-|-|-|-|-|
|GGAD|0.7514±0.0410|0.5274±0.0052|0.6114±0.0219|0.7006±0.0090|TBD|0.5382±0.0065|TBD
|GGADFormer|0.9324±0.0189|0.5629±0.0161|0.8183±0.0202|0.7221±0.0441|0.9077±0.0039|0.6534±0.0195|0.5568±0.0147

AP:

|Dataset|Amazon|Reddit|photo|elliptic|t_finance|tolokers|questions
|-|-|-|-|-|-|-|-|
|GGAD|0.3755±0.0749|0.0360±0.0003|0.1269±0.0091|0.2565±0.0200|TBD|0.2448±0.0039|TBD
|GGADFormer|0.8080±0.0088|0.0418±0.0042|0.4756±0.0585|0.2268±0.0755|0.6589±0.0323|0.3063±0.0138|0.0375±0.0020
