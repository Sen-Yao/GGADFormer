# Introduction

图异常检测（Graph Anomaly Detection, GAD）是一项旨在识别网络数据中罕见实体或事件的关键任务，对金融风控、网络安全等领域至关重要。然而，将 GAD 应用于现实世界的过程中，两大瓶颈尤为突出：工业级图的海量规模与标注数据的极度稀缺。因此，仅需少量已知“正常”样本的半监督学习范式，已成为最具实用价值的研究前沿。尽管如此，如何设计一个既能高效处理大规模图、又不过分依赖稀有标签的算法，依然是一个悬而未决的核心挑战。

现有的半监督图异常检测模型大多基于图神经网络（GNN）实现。GNN 的核心机制——消息传递，要求在训练和推理过程中反复进行邻居采样与多层信息聚合。这一过程深度依赖于两个独立的、巨大的数据结构：描述拓扑的邻接矩阵和承载节点属性的特征矩阵。这种范式在处理大规模图时，面临着严峻的可扩展性挑战。首先，随着 GNN 层数的加深或节点度的增加，计算单个节点表示所需的邻居数量呈指数级增长，即所谓的“邻居爆炸”（Neighborhood Explosion）问题。这不仅导致了巨大的显存开销，使得将工业级大图完整加载到计算设备中变得不切实际，更引发了因邻域高度重叠造成的计算冗余，严重影响训练效率。此外，图神经网络自身普遍受限于过平滑与过拟合现象，制约了模型在长距离关系引发的异常的捕捉能力。尽管学术界已提出多种基于采样的训练策略以缓解此问题，但它们并未从根本上解决对显式图结构的依赖。这些方法通过处理子图来降低单次迭代的复杂度，却不可避免地引入了信息损失和训练方差，可能破坏对异常检测至关重要的全局结构信息。同时，复杂的图划分和采样预处理也带来了额外的开销。为了将图异常检测技术真正应用于大规模现实场景，可扩展性（Scalability）是一个无法回避的核心挑战。

另一方面，在工业场景中，尽管半监督图异常检测设定（即拥有少量正常标签）比无监督设定更贴近实际，但获取标签的成本和难度依然是阻碍算法落地的核心瓶颈。标记数据，尤其是高质量的标记数据，需要大量的专家知识和时间投入。在异常检测领域，这个问题尤为突出，因为异常事件本身具有稀有性、多变性和动态演化性，使得标注工作极其困难且昂贵。例如，在电子商务欺诈检测中，欺诈模式快速演变，需要持续的专家标注，而“标签稀缺”正是该领域亟待解决的关键挑战之一。现有研究中广泛采用的标签比例（例如，15% 的正常节点）虽然为算法比较提供了一个公平的基准，但这一比例在许多真实工业应用中仍然是过于理想化的。在现实世界中，可信赖的标签可能远低于这个水平，甚至不足 1%。为了解决标签数据的稀缺问题，近年来，基于生成式（Generative）的模型取得了一定的效果。它们的核心思路在于，基于现有的正常节点，人为地施加干扰，将干扰后的结果作为生成的伪异常节点（负样本）进行训练。然而，由于对异常模式的先验知识受限，现有的生成式方法大多通过对正常节点嵌入施加随机噪声或基于邻居节点特征进行聚合。显然，这类方法的成败在很大程度上取决于负样本的质量。一方面，如果模型生成的伪异常嵌入过于接近正常模式，那么将其作为异常样本训练时会损害模型对正常模式的学习，使得正常类别的嵌入空间分布变得松散，决策边界模糊。另一方面，对于与当前正常样本差异巨大的伪异常点，这种“简单负样本”也无法给模型带来足够大的挑战。这并不能帮助模型学到一个紧凑而精确的决策边界。模型会满足于找到一个“捷径解 (shortcut solution)”，而没有真正理解正常与异常之间细微的差异。因此，现有的生成式方法缺乏现有生成式方法缺乏对“什么是有效异常”的可靠指导信号，无法确保其偏离方向具有语义意义。

在这方面，无监督方法，特别是基于重构的模型提供了一个富有启发性的视角。这类方法基于正常性假设 (Normality Assumption)，即模型经正常样本训练后，能够准确重构正常节点，而对异常节点的重构则会产生较大偏差。因此，重构误差的幅度便被用作异常判别的准则。然而，这种做法存在一个关键的信息瓶颈：它将丰富的高维偏差信息压缩为了一个单一的标量值。一个高的重构误差仅仅表明输入与模型所学习的“正常性流形 (Normality Manifold)”不符，但它并未揭示样本是在哪个“方向”上、以何种方式偏离了正常模式。我们将这个描述偏差方向与结构的完整误差向量称为结构化重构误差 (Structured Reconstruction Error)。我们认为，这个被传统方法所忽略的向量，实际上蕴含着生成高质量负样本的宝贵指导信息，是一个极具潜力的中间信号 (intermediate signal)

这一洞察启发我们提出一个根本性的问题：能否将结构化重构误差从一个被动的度量指标，转变为一个主动的生成向导，从而在无需访问显式图结构的前提下，高效地合成能够最大化挑战模型决策边界的“边界感知”硬负样本 (boundary-aware hard negatives)？

为实现这一目标，我们设计并提出了生成式图异常检测的图 Transformer (Generative Graph Anomaly Detection Transformer, GGADFormer)。该框架巧妙地利用 Transformer 自编码器的结构，在对正常节点进行表征学习的同时，捕获其结构化重构误差。随后，它将此高维误差向量跨空间投影 (cross-space projection) 至嵌入空间，将其转化为一个有意义的、指向“最可疑”异常方向的扰动向量。通过将正常节点的嵌入沿着此方向进行偏移，GGADFormer 能够自适应地生成高质量的伪异常样本。这一“自我挑战”的闭环机制，迫使模型学习到一个对正常模式更紧凑、更精确的表征。

GGADFormer 的主要贡献概括如下：

- 提出一种可扩展的、解耦式的图表示学习架构。 我们设计了一种图结构感知的输入编码策略，它将图拓扑信息的提取与下游的节点表征学习分离。这使得模型能够彻底摆脱传统 GNN 对实时消息传递的依赖，从而支持高效的小批量训练，天然地适用于大规模工业级图数据。
- 开创一种基于重构误差的自适应负样本生成策略。 我们创新性地提出，利用 Transformer 解码器产生的结构化重构误差来指导伪异常的生成。这种跨空间生成策略，能够根据模型当前的认知，为正常样本自适应地创造位于决策边界附近的硬负样本，从而显著提升了模型的判别能力和对复杂异常的泛化能力。
- 系统性地验证了模型在极端标签稀缺场景下的有效性。 我们的方法专为真实工业场景中标签极度稀缺的挑战而设计。通过在多个基准数据集上的大量实验，我们证明了 GGADFormer 在训练标签比例远低于传统设定的严苛条件下，依然能够取得卓越的性能，填补了该领域在实际应用场景下的研究空白。

# 模型架构

在此节我们将介绍 GGADFormer 的各个组成部分。

## 图结构感知的 Tokenization

传统基于消息传递的图神经网络（GNN）在大规模图异常检测中存在两大结构性瓶颈：一是随着层数和节点度的增加，邻居采样会带来指数级膨胀的计算和显存开销（即“邻居爆炸”问题）；二是多层聚合导致节点表示逐渐趋同，过度平滑效应使得模型难以捕捉异常所需的细微差异。这些限制严重制约了 GNN 在工业级场景下的可扩展性与判别能力。

为了突破这一瓶颈，我们设计了一种预计算、解耦式的图结构感知 Tokenization 策略。该方法的核心思想是：在模型输入阶段就将图的拓扑上下文显式编码为一系列 token，而非依赖训练过程中的动态邻居采样与多层消息传递。这样一来，图的结构信息与节点的特征信息被解耦，Transformer 可以直接在 token 序列上进行建模，从根本上避免了邻居爆炸和重复计算。具体而言，我们通过一个轻量级、可并行化的传播过程，预先为每个节点生成一个融合多跳邻域信息的结构化 token 序列：

$$X_p^k = \hat {A}^k\cdot \mathbf{X}_0$$

$$X_p^{k+1} = (1-\alpha)\cdot X_p^{k} + \alpha \cdot \mathbf{X}_0$$

其中，$\hat{A}$ 是归一化后的邻接矩阵，$\mathbf{X}_0$ 是节点的原始特征， $\alpha\in (0,1)$ 为保留原始特征的残差系数。经过 $k$ 次传播后，每个节点便对应一个 token 序列 $X_0, X_p^1, X_p^2,\cdots, X_p^k$，这一序列不仅逐层融合了邻居的拓扑语境，还在 $\alpha$ 的调节下保留了节点的原始特征，避免了过度平滑导致的同质化。换言之，节点的独特性与邻域的结构语义得以在 token 空间中共同表达。

这种 Tokenization 带来两个关键优势： 1. **可扩展性**：由于 Transformer 只需处理 token 序列而非全图邻域，训练过程可以天然支持小批量（mini-batch），避免了显存瓶颈，使得在工业级大图上依然保持高效。 2. **表达力**：token 序列为异常检测提供了比单一嵌入更丰富的上下文刻画，尤其能够捕捉那些依赖于长程关系或局部细微差异的复杂异常模式。 因此，图结构感知的 Tokenization 并非单纯的输入预处理，而是我们整个框架得以兼顾“可扩展性”与“判别力”的基础环节。

## 基于跨空间重构误差引导的伪异常生成

传统的基于生成式的半监督图异常检测方法通常依赖已知的正常样本嵌入，通过对其施加扰动（例如启发式地添加高斯噪声，或结合节点特征与拓扑结构）来合成负样本，从而增强模型的泛化能力。然而，在正常标签极度稀缺的现实场景中，这类方法面临双重挑战：一方面，可用于学习正常模式的样本极为有限，导致所学嵌入表征不充分；另一方面，模型对异常模式缺乏可靠的先验知识，难以判断何种扰动方向能生成“语义上有意义”的异常。其结果是，生成的负样本质量参差不齐——要么过于接近正常分布，削弱模型对正常类别的紧凑建模；要么偏离过远，沦为缺乏判别挑战性的“简单负样本”。更关键的是，现有方法在构造异常点时往往依赖显式的图拓扑结构（如邻接矩阵），需在训练过程中动态访问大规模图数据，这不仅引入额外计算开销，更严重制约了生成式方法在工业级大图场景中的可扩展性与实用性。

为了突破这一瓶颈，我们提出了一个基于自编码器思想的协同框架，使其在表征学习过程中既发挥正则化器 (Representation Regularizer) 的作用，又作为伪异常生成引导者 (Outlier Generation Guidance) 提供主动挑战。

该框架的核心在于一个**编码器-解码器**结构。我们的 Transformer 编码器 $\mathcal{E}$ 负责将输入令牌 $\mathbf{t}_i$ (包含节点 $v_i$ 的原始及结构化特征) 在多层注意力机制下压缩成一个低维、稠密的嵌入表征 $\mathbf{h}_i = \mathcal{E}(\mathbf{t}_i)$。而解码器 $\mathcal{D}_{tok}$，其任务是尝试从嵌入 $\mathbf{h}_i$ “还原”回原始的输入令牌 $\hat{\mathbf{t}}_i = \mathcal{D}_{tok}(\mathbf{h}_i)$。我们认为，一个高质量、信息丰富的节点嵌入，理应蕴含足够的信息来还原其自身的全部构成。由于训练数据主要由正常节点构成，最小化重构误差的过程会“迫使”编码器 $\mathcal{E}$ 去学习和提炼正常模式最本质、最关键的特征，从而在噪声与冗余中提炼出一个更为纯净的“正常性流形 (Normality Manifold)”表示，从而使最终的嵌入 $\mathbf{h}_i$ 成为对“正常性”的高度浓缩和纯化的表达。这一过程不仅提升了嵌入的判别力，也为异常建模提供了一个稳定的语义基准。

然而，此重构机制的价值远不止于表征正则化。其产生的**重构误差向量**，通常被视为一个待优化的标量损失，实则蕴藏了用以生成高质量伪异常的关键信息。我们理论的出发点是**流形假设**，即正常节点的数据分布在嵌入空间中构成一个低维的“正常性流形”。解码器 $\mathcal{D}_{tok}$ 的存在隐式地定义了这个流形。对于一个正常节点 $v_i$，其在令牌空间的重构误差向量被定义为：
$$ \mathbf{e}^{(tok)}_i = \mathcal{D}_{tok}(\mathbf{h}_i) - \mathbf{t}_i $$
该向量 $\mathbf{e}^{(tok)}_i$ 精确地指明了在模型的当前认知下，样本 $v_i$ 的哪些方面最不符合其学到的“通用正常模式”。它是在高维、结构化的令牌空间中对“异常倾向”的直接量化。

为了在低维、稠密的嵌入空间中利用这一信息，我们引入了一个可学习的线性**投影层** $\mathcal{P}: \mathbb{R}^{d_{token}} \to \mathbb{R}^{d_{emb}}$，它负责将令牌空间中的结构化误差向量 $\mathbf{e}^{(tok)}_i$ **“语义提升”** (semantically lift) 为嵌入空间中的一个有意义的扰动方向 $\mathbf{e}^{(emb)}_i$：
$$ \mathbf{e}^{(emb)}_i = \mathcal{P}(\mathbf{e}^{(tok)}_i) $$
最终，我们通过将一个正常节点的原始嵌入 $\mathbf{h}_i$，沿着这个由模型自身指认的“最可疑”的异常方向 $\mathbf{e}^{(emb)}_i$ 进行扰动，来合成其对应的伪异常嵌入 $\tilde{\mathbf{h}}_i$：
$$ \tilde{\mathbf{h}}_i = \mathbf{h}_i + \beta \cdot \mathbf{e}^{(emb)}_i $$
其中，$\beta$ 是一个超参数，用于控制所生成异常的强度。这种生成方式生成的不是随机的或基于简单启发式规则的负样本，而是针对模型当前认知边界的**自适应硬负样本 (Adaptive Hard-Negative Samples)**。通过这种方式，自编码器框架形成了一个协同增强的闭环：解码器在作为正则化器的同时，主动地为对比学习模块提供了最具挑战性的学习信号，从而驱动整个模型学习一个更加鲁棒和紧凑的正常类别边界。

## 中心点对齐

尽管我们设计的生成策略能够构造出与正常样本显著不同的伪异常点，但若不对其在嵌入空间中的位置施加约束，优化过程极易陷入一种“捷径解”（shortcut solution）：模型仅需将伪异常嵌入推至远离正常点簇的无穷远处，即可轻易实现分类分离。虽然这种无约束的分离能在训练损失上取得表面优势，却会导致所学决策边界松散、泛化能力薄弱，无法精准刻画“正常”与“异常”之间那条微妙而关键的分界线。

为克服这一问题，我们引入 中心点对齐（Central Point Alignment, CPA） 机制，其核心思想是：将所有生成的伪异常样本约束在一个以正常点簇中心为球心、预设半径为界的“可信异常超球”（Credible Anomaly Hypersphere）内，从而确保它们位于决策边界附近，成为真正具有判别挑战性的硬负样本。

具体而言，我们首先动态计算当前训练批次中所有正常节点嵌入的均值，作为正常模式的原型中心（prototypical center） $\mathbf{c}$：
$$ \mathbf{c} = \frac{1}{|V_{norm}|} \sum_{v_i \in V_{norm}} \mathbf{h}_i $$
其中，$V_{norm}$ 是当前批次中的正常节点集合，$\mathbf{h}_i$ 是其对应的嵌入。这个中心点 $\mathbf{c}$ 代表了模型在当前状态下对“普遍正常性”的凝聚表达。

在此基础上，我们定义一个基于边距 (margin) 的对齐损失 $L_{CPA}$，用于约束伪异常样本的空间分布。对于每一个生成的伪异常嵌入 $\tilde{\mathbf{h}}_j$，我们计算其到中心点 $\mathbf{c}$ 的欧氏距离，并对超出预设信心边距 $R$ (confidence margin) 的样本施加惩罚：
$$ L_{CPA} = \frac{1}{|\tilde{V}|} \sum_{\tilde{\mathbf{h}}_j \in \tilde{V}} \max(0, \ \|\tilde{\mathbf{h}}_j - \mathbf{c}\|_2 - R) $$
其中, $\tilde{V}$ 是生成的伪异常样本集合。此损失函数具有明确的几何意义：当一个伪异常点位于以 $\mathbf{c}$ 为中心、半径为 $R$ 的超球内部或表面时（$\|\tilde{\mathbf{h}}_j - \mathbf{c}\|_2 \le R$），不施加任何惩罚；一旦其越出该边界，损失将随其距离的增加而线性增长。

通过这一机制，中心点对齐有效防止了伪异常样本在嵌入空间中的无序扩散。生成的负样本既不会因过于接近正常簇而丧失判别价值，也不会因被推向遥远区域而沦为平凡解。相反，它们被精准锚定在正常模式的“边缘地带”，持续为模型提供高信息量、高挑战性的学习信号，从而驱动模型学习到一个更紧凑、更精确且更具泛化能力的异常检测边界。

## 异常检测模块

在获得高质量的节点嵌入表示后，我们引入一个轻量级的判别头（discriminative head）用于最终的异常评分。该模块以 Transformer 编码器输出的节点嵌入 $h_i\in R^{d_{emb}}$，为输入，通过一个多层感知机（MLP）将其映射为一个标量异常分数 $s_i\in[0,1]$，表示节点 $v_i$ 被判定成异常的可能性

$$s_i=\sigma(MLP(h_i;\Theta_i))$$

其中 $\sigma$ 为 sigmoid 激活函数，$\Theta_i$ 表示判别头的可学习参数。

在训练阶段，我们采用伪标签对比学习策略构建监督信号。具体而言，我们将两类样本共同构成训练集。来自标注集合 $V_{norm}$ 的已知正常节点；而模型基于重构误差生成的伪异常节点嵌入 $\tilde h_i$，对应节点为 $\tilde {v}_j$

我们为每类样本分配确定性标签：对真实正常节点 $v_i\in V_{norm}$，设其标签 $y_i=1$；对生成的伪异常节点 $\tilde v_j$，设 $y_j=0$ 。基于此，我们采用二元交叉熵（Binary Cross-Entropy, BCE）损失优化判别头：

$$L_{BCE}=-\frac{1}{|V_{train}|}\sum _{v\in V_{train}}[y_k\log (s_k)+(1-y_k)\log (1-s_k)]$$

其中 $V_{train}=V_{norm}\cup \tilde{V}$ 为当前训练批次中的全部样本，$\tilde{V}$ 为生成的伪异常节点集合。值得注意的是，该判别模块并非孤立训练，而是与前文所述的自编码重构模块和中心点对齐机制端到端联合优化。整体训练目标由三项损失加权组成：

$$L_{total}=L_{BCE}+\lambda_{rec}\cdot L_{REC}+\lambda_{cpa}\cdot L_{CPA}$$

其中 $L_{REC}$ 为令牌级重构损失，用于驱动编码器学习紧致的正常性表征；$L_{CPA}$ 为中心点对齐损失，用于约束伪异常的空间分布；$\lambda_{rec},\lambda_{cpa}>0$ 为超参数，用于平衡各目标的重要性。在推理阶段，模型仅需前向传播一次即可获得所有节点的异常分数 $s_i$。由于我们的架构完全解耦了图结构与表征学习过程，推理可高效地以 mini-batch 方式进行，天然支持大规模图场景。

该设计实现了无需显式图遍历、仅依赖少量正常标签、且能自适应生成高挑战性负样本的异常检测流程，显著提升了模型在真实工业场景中的实用性与鲁棒性。

## Experiments

我们在 `Amazon`, `photo`, `reddit`, `ellptic`, `t_finance`, `tolokers`, `questions` 等多个不同的图异常检测常用的数据集上进行了试验。

数据集划分上，我们仅采用 5% 的节点作为训练集，模型仅能访问训练集中的正常节点的标签。

### 实验对比

我们使用了 5 个随机种子进行了实验，实验结果如下：

AUC:


|Dataset|Amazon|Reddit|photo|elliptic|t_finance|tolokers|questions
|-|-|-|-|-|-|-|-|
|GGAD|0.7514±0.0410|0.5274±0.0052|0.6114±0.0219|0.7006±0.0090|TBD|0.5382±0.0065|TBD
|GGADFormer|0.9324±0.0189|0.5629±0.0161|0.8183±0.0202|0.7221±0.0441|0.9077±0.0039|0.6534±0.0195|0.5568±0.0147

AP:

|Dataset|Amazon|Reddit|photo|elliptic|t_finance|tolokers|questions
|-|-|-|-|-|-|-|-|
|GGAD|0.3755±0.0749|0.0360±0.0003|0.1269±0.0091|0.2565±0.0200|TBD|0.2448±0.0039|TBD
|GGADFormer|0.8080±0.0088|0.0418±0.0042|0.4756±0.0585|0.2268±0.0755|0.6589±0.0323|0.3063±0.0138|0.0375±0.0020
