# Introduction

图异常检测（Graph Anomaly Detection）的核心任务是识别在图结构数据中行为模式或特征显著偏离大部分节点的实体。这些异常通常表现为结构性异常（如孤立点或意外的社群连接）、属性异常（节点的特征向量与众不同），亦或是两者的结合。在众多现实场景中，标签稀缺是一个普遍挑战，由此催生了半监督图异常检测（Semi-supervised Graph Anomaly Detection）这一重要研究方向。该设定下，模型在训练时仅能利用极少数已知的“正常”节点作为先验知识，而绝大部分正常节点与所有的异常节点均是无标签的。考虑到真实世界应用（如金融风控、网络安全入侵检测）中数据规模庞大、异常模式未知多变，使得全面标注的成本极其高昂。与之相对，获取少量正常样本的成本则低廉得多。因此，半监督图异常检测范式不仅具备高度的普适性，更是在资源受限的实际环境中解决问题的关键技术，具有重要的理论研究与实际应用价值。图异常检测 (Graph Anomaly Detection, GAD) 是一项旨在识别网络数据中罕见实体的关键任务，对金融风控、网络安全等领域至关重要。然而，在将 GAD应用于现实世界的过程中，两大瓶颈尤为突出：工业级图的海量规模与标注数据的极度稀缺。因此，仅需少量已知“正常”样本的半监督学习范式，已成为最具实用价值的研究前沿。尽管如此，如何设计一个既能高效处理大规模图、又不过分依赖稀有标签的算法，依然是一个悬而未决的核心挑战。

现有的半监督图异常检测模型大多基于图神经网络（GNN）实现。GNN 的核心机制——消息传递，要求在训练和推理过程中反复进行邻居采样与多层信息聚合。这一过程深度依赖于两个独立的、巨大的数据结构：一个描述拓扑的邻接矩阵和一个承载节点属性的特征矩阵。这种范式在处理大规模图时，面临着严峻的可扩展性挑战。首先，随着 GNN 层数的加深或节点度的增加，计算单个节点表示所需的邻居数量呈指数级增长，即所谓的“邻居爆炸”问题。这不仅导致了巨大的显存开销，使得将工业级大图完整加载到计算设备中变得不切实际，更引发了因邻域高度重叠造成的计算冗余，严重影响训练效率。此外，图神经网络自身普遍受限于过平滑与过拟合现象。尽管学术界已提出多种基于采样的训练策略以缓解此问题，但它们并未从根本上解决对显式图结构的依赖。这些方法通过处理子图来降低单次迭代的复杂度，却引入了信息损失和训练方差，可能破坏对异常检测至关重要的全局结构信息。同时，复杂的图划分和采样预处理也带来了额外的开销。为了将图异常检测技术真正应用于大规模现实场景，可扩展性 (Scalability) 是一个无法回避的核心挑战。

另一方面在工业场景中，尽管半监督图异常检测设定（即拥有少量正常标签）比无监督设定更贴近实际，但获取标签的成本和难度依然是阻碍算法落地的核心瓶颈。标记数据，尤其是高质量的标记数据，需要大量的专家知识和时间投入。在异常检测领域，这个问题尤为突出，因为异常事件本身具有稀有性和多变性，使得标注工作极其困难且昂贵。例如，在电子商务欺诈检测中，欺诈模式快速演变，需要持续的专家标注，而“标签稀缺”正是该领域需要解决的关键挑战之一。现有研究中广泛采用的标签比例（例如，15%的正常节点）虽然为算法比较提供了一个公平的基准，但这一比例在许多真实工业应用中仍然是过于理想化的。在现实世界中，可信赖的标签可能远低于这个水平，甚至不足1%。为了解决标签数据的稀缺问题，近年来，基于生成式 (Generative) 的模型取得了比较好的效果。它们的核心思路在于，基于现有的正常节点，人为地对其施加干扰，将干扰后的结果作为生成的伪异常节点作为负样本。然而由于对异常模式的知识受限，现有的生成式方法大多通过对正常节点嵌入施加随机噪声或基于邻居节点特征进行聚合。显然，这类方法的成败在很大程度上取决于负样本的质量。一方面，模型生成的伪异常嵌入如果不够异常，那么将其作为异常样本训练时会损害模型对正常模式的学习，使得正常类别的嵌入空间分布变得松散，边界模糊。而对于与当前正常样本差异巨大的伪异常点，这种“简单负样本”也无法给模型带来足够大的挑战。这并不能帮助模型学到一个紧凑而精确的决策边界。模型会满足于找到一个“捷径解 (shortcut solution)”，而没有真正理解正常与异常之间细微的差异。

在无监督异常检测中的一种主流方法是基于重构 (Reconstruction) 的模型，比如图自编码器 (Graph Autoencoder)。它们的核心假设是：模型在只见过正常样本的训练后，能够很好地重构正常节点，但无法很好地重构异常节点。因此，重构误差的大小就成了判断异常的依据。但问题在于个很高的重构误差，仅仅说明了输入数据与模型学到的“正常模式分布”不符。但这个标量误差本身，并没有在嵌入空间中为“异常”提供一个明确的、可分离的表征。传统生成式模型将重构误差作为最终的异常分数，但这忽略了其作为中间信号的巨大潜力。

理想的负样本，应该位于正常类别簇的边界附近，能够最大程度地挑战模型当前的决策边界，同时避免生成负样本时对显式图结构的实时访问，从而改进模型在大图上的可扩展性。这一观察启发我们提出一个根本性的问题，能否将重构误差带来的丰富信息作为负样本生成的依据，在训练和推理时无需显式读取图结构的同时高效完成高质量的负样本生成？

为系统性地应对上述挑战，我们设计并提出了“生成式图异常检测的图 Transformer” (Generative Graph Anomaly Detection Transformer, GGADFormer) ，在训练样本极度稀缺的场景下，通过利用 Transformer 自编码器的无监督重构误差，高效地完成高质量的负样本生成，提高了模型的生成质量。

GGADFormer 的先进性主要体现在其四大核心贡献：1) 一种解耦式的图结构感知输入编码策略，它将图拓扑信息的提取与节点表征学习分离，使得模型无需显式访问完整图结构，能够进行高效的小批量训练，天然地适用于大规模图；2) 一种新颖的生成式策略，在缺乏真实异常样本的情况下，**人工合成高质量的伪异常样本**；4) 多重学习机制，确保模型学习到鲁棒且具有区分性的节点表征。

GGADFormer 的核心贡献包括

- 我们提出了解耦式的图异常结构感知输入编码策略，它解决了传统消息传递机制在异常检测中的长距离依赖问题，从而显著提高了模型的泛化能力；并将图拓扑信息的提取与节点表征学习分离，使得模型无需显式访问完整图结构，能够进行高效的小批量训练，天然地适用于大规模图。
- 我们提出了一种新颖的基于重构误差的跨空间负样本生成式策略，在不访问完整图结构的情况下，高效地生成高质量的负样本。这种策略能够显著提高模型的泛化能力。最大程度地挑战模型当前的决策边界。
- 我们的方法面向现实场景中标签极度稀缺的场景，填补了极低划分比下的半监督图异常检测的空白

# 模型架构

在此节我们将介绍 GGADFormer 的各个组成部分。

### 图结构感知的 Tokenization

传统的图神经网络（如 GCN）通过邻居聚合来学习节点表示，但这通常会导致过度平滑（over-smoothing）问题，即随着传播层数增加，节点的表示趋于同质化，难以区分，这对需要捕捉细微差异的异常检测任务是致命的。

为解决此问题并让 Transformer 感知到图结构信息，我们采用一种基于传播机制来编码节点的拓扑特征。

$$X_p^k = \hat {A}^k\cdot \mathbf{X}_0$$

$$X_p^{k+1} = (1-\alpha)\cdot X_p^{k} + \alpha \cdot \mathbf{X}_0$$

其中，$\hat{A}$ 是归一化后的邻接矩阵，$\mathbf{X}_0$ 是节点的原始特征。经过 $k$ 次传播后，得到了一系列的 token sequence $X_0, X_p^1, X_p^2,\cdots, X_p^k$，此序列融入了节点邻居的结构信息，同时超参数 $\alpha$ 保证了节点自身的原始特征不会被完全稀释，从而有效缓解了过度平滑问题，保留了节点的独特性。

token sequence 保留了用于对应节点用于异常检测所需要的所有图上下文信息，因此 Transformer 仅需要处理此序列，不需要计算全图中的节点信息，大大降低了计算开销。

## 基于跨空间重构误差引导的伪异常生成

在半监督学习中，尤其当已标记的正常节点极为稀疏时，单纯依赖对比学习机制的模型面临着学习“捷径解”的风险。模型可能仅学会区分有限的训练样本，而未能泛化至对“正常性”这一抽象概念的全面理解。为了应对这一挑战并从根本上提升节点表征的质量，我们设计了一个基于自编码器思想的协同框架，它不仅作为一种强大的**表征正则化 (Representation Regularization)** 手段，更创新性地扮演了**伪异常生成引导者 (Outlier Generation Guidance)** 的角色。

该框架的核心在于一个**编码器-解码器**结构。我们的Transformer编码器 $\mathcal{E}$ 负责将输入令牌 $\mathbf{t}_i$ (包含节点 $v_i$ 的原始及结构化特征) 压缩成一个低维、稠密的嵌入表征 $\mathbf{h}_i = \mathcal{E}(\mathbf{t}_i)$。我们额外设计了一个解码器 $\mathcal{D}_{tok}$，其任务是从嵌入 $\mathbf{h}_i$ “还原”回原始的输入令牌 $\hat{\mathbf{t}}_i = \mathcal{D}_{tok}(\mathbf{h}_i)$。我们认为，一个高质量、信息丰富的节点嵌入，理应蕴含足够的信息来还原其自身的全部构成。由于训练数据主要由正常节点构成，最小化重构误差的过程会“迫使”编码器 $\mathcal{E}$ 去学习和提炼正常模式最本质、最关键的特征，自然地过滤掉随机噪声，从而使最终的嵌入 $\mathbf{h}_i$ 成为对“正常性”的高度浓缩和纯化的表达。

然而，此重构机制的价值远不止于表征正则化。其产生的**重构误差向量**，通常被视为一个待优化的标量损失，实则蕴藏了用以生成高质量伪异常的关键信息。我们理论的出发点是**流形假设**，即正常节点的数据分布在嵌入空间中构成一个低维的“正常性流形”。解码器 $\mathcal{D}_{tok}$ 的存在隐式地定义了这个流形。对于一个正常节点 $v_i$，其在令牌空间的重构误差向量被定义为：
$$ \mathbf{e}^{(tok)}_i = \mathcal{D}_{tok}(\mathbf{h}_i) - \mathbf{t}_i $$
该向量 $\mathbf{e}^{(tok)}_i$ 精确地指明了在模型的当前认知下，样本 $v_i$ 的哪些方面最不符合其学到的“通用正常模式”。它是在高维、结构化的令牌空间中对“异常倾向”的直接量化。

为了在低维、稠密的嵌入空间中利用这一信息，我们引入了一个可学习的线性**投影层** $\mathcal{P}: \mathbb{R}^{d_{token}} \to \mathbb{R}^{d_{emb}}$，它负责将令牌空间中的结构化误差向量 $\mathbf{e}^{(tok)}_i$ **“语义提升”** (semantically lift) 为嵌入空间中的一个有意义的扰动方向 $\mathbf{e}^{(emb)}_i$：
$$ \mathbf{e}^{(emb)}_i = \mathcal{P}(\mathbf{e}^{(tok)}_i) $$
最终，我们通过将一个正常节点的原始嵌入 $\mathbf{h}_i$，沿着这个由模型自身指认的“最可疑”的异常方向 $\mathbf{e}^{(emb)}_i$ 进行扰动，来合成其对应的伪异常嵌入 $\tilde{\mathbf{h}}_i$：
$$ \tilde{\mathbf{h}}_i = \mathbf{h}_i + \alpha \cdot \mathbf{e}^{(emb)}_i $$
其中，$\alpha$ 是一个超参数，用于控制所生成异常的强度。这种生成方式生成的不是随机的或基于简单启发式规则的负样本，而是针对模型当前认知边界的**自适应硬负样本 (Adaptive Hard-Negative Samples)**。通过这种方式，自编码器框架形成了一个协同增强的闭环：解码器在作为正则化器的同时，主动地为对比学习模块提供了最具挑战性的学习信号，从而驱动整个模型学习一个更加鲁棒和紧凑的正常类别边界。

## 中心点对齐：构建可信异常边界

尽管我们设计的生成策略能够创造出与正常样本显著不同的伪异常点，但若不对其在嵌入空间中的位置加以约束，优化过程可能会陷入一个“捷径解”：模型仅仅学会将伪异常点的嵌入推向距正常点簇无穷远的位置，从而轻易地实现分离。这种无约束的分离虽然能降低损失，但会导致模型学习到一个松散、泛化能力差的决策边界，而未能精确刻画“正常”与“异常”之间那条微妙而关键的界线。

为了解决这一问题，我们引入了**中心点对齐 (Central Point Alignment)** 机制，其核心目标是强制所有生成的伪异常样本，落在一个以正常点簇中心为球心、预设半径为界的**可信异常超球 (Credible Anomaly Hypersphere)** 内。

首先，我们通过计算训练批次中所有正常节点嵌入的均值，来动态地确定正常点簇的**原型中心 (prototypical center)** $\mathbf{c}$：
$$ \mathbf{c} = \frac{1}{|V_{norm}|} \sum_{v_i \in V_{norm}} \mathbf{h}_i $$
其中，$V_{norm}$ 是当前批次中的正常节点集合，$\mathbf{h}_i$ 是其对应的嵌入。这个中心点 $\mathbf{c}$ 代表了模型在当前状态下对“普遍正常性”的凝聚表达。

接着，我们定义一个基于边距 (margin) 的对齐损失 $L_{CPA}$。对于每一个生成的伪异常嵌入 $\tilde{\mathbf{h}}_j$，我们计算其到中心点 $\mathbf{c}$ 的欧氏距离，并惩罚那些超出预设信心边距 $R$ (confidence margin) 的样本：
$$ L_{CPA} = \frac{1}{|\tilde{V}|} \sum_{\tilde{\mathbf{h}}_j \in \tilde{V}} \max(0, \ \|\tilde{\mathbf{h}}_j - \mathbf{c}\|_2 - R) $$
其中, $\tilde{V}$ 是生成的伪异常样本集合。此损失函数具有明确的几何意义：当一个伪异常点位于以 $\mathbf{c}$ 为中心、半径为 $R$ 的超球内部或表面时（$\|\tilde{\mathbf{h}}_j - \mathbf{c}\|_2 \le R$），不施加任何惩罚；一旦其越出该边界，损失将随其距离的增加而线性增长。

通过这种方式，“中心点对齐”机制确保了生成的伪异常样本始终处于一个“可信”的异常区域内。它们既不会因与正常点簇过于接近而失去作为负样本的价值，也不会因被推到无限远处而成为优化过程中的平凡样本。相反，它们被约束在决策边界附近，为模型提供了最具挑战性与信息量的学习信号，从而迫使模型学习到一个更加紧凑、精确且鲁棒的异常检测边界。

TODO:

- Embedding 是通过注意力池化得到的


## Experiments

我们在 `Amazon`, `photo`, `reddit`, `ellptic`, `t_finance`, `tolokers`, `questions` 等多个不同的图异常检测常用的数据集上进行了试验。

数据集划分上，我们仅采用 5% 的节点作为训练集，模型仅能访问训练集中的正常节点的标签。

### 实验对比

我们使用了 5 个随机种子进行了实验，实验结果如下：

AUC:


|Dataset|Amazon|Reddit|photo|elliptic|t_finance|tolokers|questions
|-|-|-|-|-|-|-|-|
|GGAD|0.7514±0.0410|0.5274±0.0052|0.6114±0.0219|0.7006±0.0090|TBD|0.5382±0.0065|TBD
|GGADFormer|0.9324±0.0189|0.5629±0.0161|0.8183±0.0202|0.7221±0.0441|0.9077±0.0039|0.6534±0.0195|0.5568±0.0147

AP:

|Dataset|Amazon|Reddit|photo|elliptic|t_finance|tolokers|questions
|-|-|-|-|-|-|-|-|
|GGAD|0.3755±0.0749|0.0360±0.0003|0.1269±0.0091|0.2565±0.0200|TBD|0.2448±0.0039|TBD
|GGADFormer|0.8080±0.0088|0.0418±0.0042|0.4756±0.0585|0.2268±0.0755|0.6589±0.0323|0.3063±0.0138|0.0375±0.0020
